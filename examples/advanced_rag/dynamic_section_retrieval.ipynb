{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f4b2c37d-3b5a-47aa-95b9-d28e0bc83f77",
      "metadata": {
        "id": "f4b2c37d-3b5a-47aa-95b9-d28e0bc83f77"
      },
      "source": [
        "# Dynamic Section Retrieval with LlamaParse\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/run-llama/llamacloud-demo/blob/main/examples/advanced_rag/dynamic_section_retrieval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "This notebook showcases a concept called \"dynamic section retrieval\".\n",
        "\n",
        "A common problem with naive RAG approaches is that each document is hierarchically organized by section, but standard chunking/retrieval searches for chunks that can be fragments of the entire section and miss out on relevant context.\n",
        "\n",
        "Dynamic section retrieval takes into account entire contiguous sections as metadata during retrieval, avoiding the problem of retrieving section fragments.\n",
        "1. First, tag chunks of a long document with the sections they correspond to, through structured extraction.\n",
        "2. Do two-pass retrieval. After initial semantic search, dynamically pull in the entire section through metadata filtering.\n",
        "\n",
        "![](https://github.com/run-llama/llama_parse/blob/main/examples/advanced_rag/dynamic_section_retrieval_img.png?raw=1)\n",
        "\n",
        "This helps provide a solution to the common chunking problem of retrieving chunks that are only subsets of the entire section you're meant to retrieve."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e4f707a-c7b5-473f-b4a6-881e2245e82d",
      "metadata": {
        "id": "2e4f707a-c7b5-473f-b4a6-881e2245e82d"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Install core packages and download relevant files. Here we load some popular ICLR 2024 papers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "71bd0714-324f-48b3-8a93-72c6c3a10b53",
      "metadata": {
        "id": "71bd0714-324f-48b3-8a93-72c6c3a10b53"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "9aa458bc-bc8d-46fe-9a57-021dd8d9e525",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9aa458bc-bc8d-46fe-9a57-021dd8d9e525",
        "outputId": "69dfa346-ac20-4124-defa-d6f20b94687e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-index\n",
            "  Downloading llama_index-0.12.2-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting llama-index-agent-openai<0.5.0,>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_agent_openai-0.4.0-py3-none-any.whl.metadata (726 bytes)\n",
            "Collecting llama-index-cli<0.5.0,>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_cli-0.4.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting llama-index-core<0.13.0,>=0.12.2 (from llama-index)\n",
            "  Downloading llama_index_core-0.12.2-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting llama-index-embeddings-openai<0.4.0,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl.metadata (684 bytes)\n",
            "Collecting llama-index-indices-managed-llama-cloud>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.6.3-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting llama-index-legacy<0.10.0,>=0.9.48 (from llama-index)\n",
            "  Downloading llama_index_legacy-0.9.48.post4-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting llama-index-llms-openai<0.4.0,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_llms_openai-0.3.2-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-index-multi-modal-llms-openai<0.4.0,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_multi_modal_llms_openai-0.3.0-py3-none-any.whl.metadata (726 bytes)\n",
            "Collecting llama-index-program-openai<0.4.0,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_program_openai-0.3.1-py3-none-any.whl.metadata (764 bytes)\n",
            "Collecting llama-index-question-gen-openai<0.4.0,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_question_gen_openai-0.3.0-py3-none-any.whl.metadata (783 bytes)\n",
            "Collecting llama-index-readers-file<0.5.0,>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_readers_file-0.4.0-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting llama-index-readers-llama-parse>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index) (3.9.1)\n",
            "Requirement already satisfied: openai>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.54.4)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.2->llama-index) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.2->llama-index) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.2->llama-index) (3.11.2)\n",
            "Collecting dataclasses-json (from llama-index-core<0.13.0,>=0.12.2->llama-index)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.2->llama-index) (1.2.15)\n",
            "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.13.0,>=0.12.2->llama-index)\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from llama-index-core<0.13.0,>=0.12.2->llama-index)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.2->llama-index) (2024.10.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.2->llama-index) (0.27.2)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.2->llama-index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.2->llama-index) (3.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.2->llama-index) (1.26.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.2->llama-index) (11.0.0)\n",
            "Requirement already satisfied: pydantic<2.10.0,>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.2->llama-index) (2.9.2)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.2->llama-index) (2.32.3)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.2.0 (from llama-index-core<0.13.0,>=0.12.2->llama-index)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting tiktoken>=0.3.3 (from llama-index-core<0.13.0,>=0.12.2->llama-index)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.2->llama-index) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.2->llama-index) (4.12.2)\n",
            "Collecting typing-inspect>=0.8.0 (from llama-index-core<0.13.0,>=0.12.2->llama-index)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.2->llama-index) (1.16.0)\n",
            "Collecting llama-cloud>=0.1.5 (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud-0.1.6-py3-none-any.whl.metadata (814 bytes)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2.2.2)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (4.12.3)\n",
            "Collecting pypdf<6.0.0,>=5.1.0 (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index)\n",
            "  Downloading pypdf-5.1.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index)\n",
            "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.5.16-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index) (2024.9.11)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.2->llama-index) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.2->llama-index) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.2->llama-index) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.2->llama-index) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.2->llama-index) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.2->llama-index) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.2->llama-index) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.2->llama-index) (4.0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.6)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.2->llama-index) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.2->llama-index) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.2->llama-index) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.2->llama-index) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.2->llama-index) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.2->llama-index) (0.14.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (0.7.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2.10.0,>=2.7.0->llama-index-core<0.13.0,>=0.12.2->llama-index) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<2.10.0,>=2.7.0->llama-index-core<0.13.0,>=0.12.2->llama-index) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.2->llama-index) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.2->llama-index) (2.2.3)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.2->llama-index) (3.1.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.2->llama-index)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.13.0,>=0.12.2->llama-index)\n",
            "  Downloading marshmallow-3.23.1-py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2024.2)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core<0.13.0,>=0.12.2->llama-index) (1.2.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.2->llama-index) (24.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (1.16.0)\n",
            "Downloading llama_index-0.12.2-py3-none-any.whl (6.8 kB)\n",
            "Downloading llama_index_agent_openai-0.4.0-py3-none-any.whl (13 kB)\n",
            "Downloading llama_index_cli-0.4.0-py3-none-any.whl (27 kB)\n",
            "Downloading llama_index_core-0.12.2-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl (6.2 kB)\n",
            "Downloading llama_index_indices_managed_llama_cloud-0.6.3-py3-none-any.whl (11 kB)\n",
            "Downloading llama_index_legacy-0.9.48.post4-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_llms_openai-0.3.2-py3-none-any.whl (13 kB)\n",
            "Downloading llama_index_multi_modal_llms_openai-0.3.0-py3-none-any.whl (5.9 kB)\n",
            "Downloading llama_index_program_openai-0.3.1-py3-none-any.whl (5.3 kB)\n",
            "Downloading llama_index_question_gen_openai-0.3.0-py3-none-any.whl (2.9 kB)\n",
            "Downloading llama_index_readers_file-0.4.0-py3-none-any.whl (38 kB)\n",
            "Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl (2.5 kB)\n",
            "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading llama_cloud-0.1.6-py3-none-any.whl (195 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.8/195.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_parse-0.5.16-py3-none-any.whl (14 kB)\n",
            "Downloading pypdf-5.1.0-py3-none-any.whl (297 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
            "Downloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading marshmallow-3.23.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: striprtf, filetype, dirtyjson, tenacity, pypdf, mypy-extensions, marshmallow, typing-inspect, tiktoken, llama-cloud, dataclasses-json, llama-index-legacy, llama-index-core, llama-parse, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-program-openai, llama-index-question-gen-openai, llama-index\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "Successfully installed dataclasses-json-0.6.7 dirtyjson-1.0.8 filetype-1.2.0 llama-cloud-0.1.6 llama-index-0.12.2 llama-index-agent-openai-0.4.0 llama-index-cli-0.4.0 llama-index-core-0.12.2 llama-index-embeddings-openai-0.3.1 llama-index-indices-managed-llama-cloud-0.6.3 llama-index-legacy-0.9.48.post4 llama-index-llms-openai-0.3.2 llama-index-multi-modal-llms-openai-0.3.0 llama-index-program-openai-0.3.1 llama-index-question-gen-openai-0.3.0 llama-index-readers-file-0.4.0 llama-index-readers-llama-parse-0.4.0 llama-parse-0.5.16 marshmallow-3.23.1 mypy-extensions-1.0.0 pypdf-5.1.0 striprtf-0.0.26 tenacity-8.5.0 tiktoken-0.8.0 typing-inspect-0.9.0\n",
            "Requirement already satisfied: llama-index-core in /usr/local/lib/python3.10/dist-packages (0.12.2)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core) (3.11.2)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index-core) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core) (1.2.15)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core) (1.0.8)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core) (2024.10.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama-index-core) (0.27.2)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core) (3.4.2)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama-index-core) (1.26.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core) (11.0.0)\n",
            "Requirement already satisfied: pydantic<2.10.0,>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core) (2.9.2)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core) (8.5.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core) (0.8.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core) (4.12.2)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core) (1.16.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (4.0.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index-core) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index-core) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index-core) (2024.9.11)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2.10.0,>=2.7.0->llama-index-core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<2.10.0,>=2.7.0->llama-index-core) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core) (3.1.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index-core) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index-core) (3.23.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core) (1.0.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index-core) (0.14.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core) (24.2)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core) (1.2.2)\n",
            "Requirement already satisfied: llama-parse in /usr/local/lib/python3.10/dist-packages (0.5.16)\n",
            "Requirement already satisfied: click<9.0.0,>=8.1.7 in /usr/local/lib/python3.10/dist-packages (from llama-parse) (8.1.7)\n",
            "Requirement already satisfied: llama-index-core>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from llama-parse) (0.12.2)\n",
            "Requirement already satisfied: pydantic!=2.10 in /usr/local/lib/python3.10/dist-packages (from llama-parse) (2.9.2)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core>=0.11.0->llama-parse) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core>=0.11.0->llama-parse) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core>=0.11.0->llama-parse) (3.11.2)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index-core>=0.11.0->llama-parse) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core>=0.11.0->llama-parse) (1.2.15)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core>=0.11.0->llama-parse) (1.0.8)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core>=0.11.0->llama-parse) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core>=0.11.0->llama-parse) (2024.10.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama-index-core>=0.11.0->llama-parse) (0.27.2)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core>=0.11.0->llama-parse) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core>=0.11.0->llama-parse) (3.4.2)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core>=0.11.0->llama-parse) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama-index-core>=0.11.0->llama-parse) (1.26.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core>=0.11.0->llama-parse) (11.0.0)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core>=0.11.0->llama-parse) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core>=0.11.0->llama-parse) (8.5.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core>=0.11.0->llama-parse) (0.8.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core>=0.11.0->llama-parse) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core>=0.11.0->llama-parse) (4.12.2)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core>=0.11.0->llama-parse) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core>=0.11.0->llama-parse) (1.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=2.10->llama-parse) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=2.10->llama-parse) (2.23.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core>=0.11.0->llama-parse) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core>=0.11.0->llama-parse) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core>=0.11.0->llama-parse) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core>=0.11.0->llama-parse) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core>=0.11.0->llama-parse) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core>=0.11.0->llama-parse) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core>=0.11.0->llama-parse) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core>=0.11.0->llama-parse) (4.0.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index-core>=0.11.0->llama-parse) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index-core>=0.11.0->llama-parse) (2024.9.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core>=0.11.0->llama-parse) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core>=0.11.0->llama-parse) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core>=0.11.0->llama-parse) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core>=0.11.0->llama-parse) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core>=0.11.0->llama-parse) (3.1.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index-core>=0.11.0->llama-parse) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index-core>=0.11.0->llama-parse) (3.23.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core>=0.11.0->llama-parse) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core>=0.11.0->llama-parse) (1.0.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core>=0.11.0->llama-parse) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index-core>=0.11.0->llama-parse) (0.14.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core>=0.11.0->llama-parse) (24.2)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core>=0.11.0->llama-parse) (1.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install llama-index\n",
        "!pip install llama-index-core\n",
        "!pip install llama-parse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79821400-caaf-42f1-99d8-74c184c19e29",
      "metadata": {
        "id": "79821400-caaf-42f1-99d8-74c184c19e29"
      },
      "outputs": [],
      "source": [
        "# NOTE: uncomment more papers if you want to do research over a larger subset of docs\n",
        "\n",
        "urls = [\n",
        "    # \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n",
        "    # \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n",
        "    # \"https://openreview.net/pdf?id=LzPWWPAdY4\",\n",
        "    \"https://openreview.net/pdf?id=VTF8yNQM66\",\n",
        "    \"https://openreview.net/pdf?id=hSyW5go0v8\",\n",
        "    # \"https://openreview.net/pdf?id=9WD9KwssyT\",\n",
        "    # \"https://openreview.net/pdf?id=yV6fD7LYkF\",\n",
        "    # \"https://openreview.net/pdf?id=hnrB5YHoYu\",\n",
        "    # \"https://openreview.net/pdf?id=WbWtOYIzIK\",\n",
        "    \"https://openreview.net/pdf?id=c5pwL0Soay\",\n",
        "    # \"https://openreview.net/pdf?id=TpD2aG1h0D\",\n",
        "]\n",
        "\n",
        "papers = [\n",
        "    # \"metagpt.pdf\",\n",
        "    # \"longlora.pdf\",\n",
        "    # \"loftq.pdf\",\n",
        "    \"swebench.pdf\",\n",
        "    \"selfrag.pdf\",\n",
        "    # \"zipformer.pdf\",\n",
        "    # \"values.pdf\",\n",
        "    # \"finetune_fair_diffusion.pdf\",\n",
        "    # \"knowledge_card.pdf\",\n",
        "    \"metra.pdf\",\n",
        "    # \"vr_mcl.pdf\",\n",
        "]\n",
        "\n",
        "data_dir = \"iclr_docs\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = \"iclr_docs\"\n",
        "!mkdir \"{data_dir}\""
      ],
      "metadata": {
        "id": "A3tQDT3x1iF_"
      },
      "id": "A3tQDT3x1iF_",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80137d15-f22b-47eb-adce-ac295ced7e71",
      "metadata": {
        "id": "80137d15-f22b-47eb-adce-ac295ced7e71",
        "outputId": "ae5147ba-ea79-459c-aae9-2eefcebf0b24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mkdir: iclr_docs: File exists\n",
            "--2024-11-10 16:18:56--  https://openreview.net/pdf?id=VTF8yNQM66\n",
            "Resolving openreview.net (openreview.net)... 35.184.86.251\n",
            "Connecting to openreview.net (openreview.net)|35.184.86.251|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2680380 (2.6M) [application/pdf]\n",
            "Saving to: ‘iclr_docs/swebench.pdf’\n",
            "\n",
            "iclr_docs/swebench. 100%[===================>]   2.56M  7.22MB/s    in 0.4s    \n",
            "\n",
            "2024-11-10 16:18:57 (7.22 MB/s) - ‘iclr_docs/swebench.pdf’ saved [2680380/2680380]\n",
            "\n",
            "--2024-11-10 16:18:57--  https://openreview.net/pdf?id=hSyW5go0v8\n",
            "Resolving openreview.net (openreview.net)... 35.184.86.251\n",
            "Connecting to openreview.net (openreview.net)|35.184.86.251|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1244749 (1.2M) [application/pdf]\n",
            "Saving to: ‘iclr_docs/selfrag.pdf’\n",
            "\n",
            "iclr_docs/selfrag.p 100%[===================>]   1.19M  4.21MB/s    in 0.3s    \n",
            "\n",
            "2024-11-10 16:18:58 (4.21 MB/s) - ‘iclr_docs/selfrag.pdf’ saved [1244749/1244749]\n",
            "\n",
            "--2024-11-10 16:18:58--  https://openreview.net/pdf?id=c5pwL0Soay\n",
            "Resolving openreview.net (openreview.net)... 35.184.86.251\n",
            "Connecting to openreview.net (openreview.net)|35.184.86.251|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4775879 (4.6M) [application/pdf]\n",
            "Saving to: ‘iclr_docs/metra.pdf’\n",
            "\n",
            "iclr_docs/metra.pdf 100%[===================>]   4.55M  4.06MB/s    in 1.1s    \n",
            "\n",
            "2024-11-10 16:19:00 (4.06 MB/s) - ‘iclr_docs/metra.pdf’ saved [4775879/4775879]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!mkdir \"{data_dir}\"\n",
        "for url, paper in zip(urls, papers):\n",
        "    !wget \"{url}\" -O \"{data_dir}/{paper}\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "974ce0a5-931a-4c1f-b8f3-af670c08eb0f",
      "metadata": {
        "id": "974ce0a5-931a-4c1f-b8f3-af670c08eb0f"
      },
      "source": [
        "#### Define LLM and Embedding Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U llama-index-llms-azure-inference\n",
        "!pip install -U llama-index-embeddings-azure-inference"
      ],
      "metadata": {
        "collapsed": true,
        "id": "-8o4G_27yWFv",
        "outputId": "7d7f7e77-f3d2-4241-d282-b4983d416450",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "-8o4G_27yWFv",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-index-llms-azure-inference\n",
            "  Downloading llama_index_llms_azure_inference-0.3.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-llms-azure-inference) (3.11.2)\n",
            "Collecting azure-ai-inference>=1.0.0b5 (from llama-index-llms-azure-inference)\n",
            "  Downloading azure_ai_inference-1.0.0b6-py3-none-any.whl.metadata (31 kB)\n",
            "Collecting azure-identity<2.0.0,>=1.15.0 (from llama-index-llms-azure-inference)\n",
            "  Downloading azure_identity-1.19.0-py3-none-any.whl.metadata (80 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.6/80.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: llama-index-core<0.13.0,>=0.12.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-llms-azure-inference) (0.12.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.10.0->llama-index-llms-azure-inference) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.10.0->llama-index-llms-azure-inference) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.10.0->llama-index-llms-azure-inference) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.10.0->llama-index-llms-azure-inference) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.10.0->llama-index-llms-azure-inference) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.10.0->llama-index-llms-azure-inference) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.10.0->llama-index-llms-azure-inference) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.10.0->llama-index-llms-azure-inference) (4.0.3)\n",
            "Collecting isodate>=0.6.1 (from azure-ai-inference>=1.0.0b5->llama-index-llms-azure-inference)\n",
            "  Downloading isodate-0.7.2-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting azure-core>=1.30.0 (from azure-ai-inference>=1.0.0b5->llama-index-llms-azure-inference)\n",
            "  Downloading azure_core-1.32.0-py3-none-any.whl.metadata (39 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from azure-ai-inference>=1.0.0b5->llama-index-llms-azure-inference) (4.12.2)\n",
            "Requirement already satisfied: cryptography>=2.5 in /usr/local/lib/python3.10/dist-packages (from azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-inference) (43.0.3)\n",
            "Collecting msal>=1.30.0 (from azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-inference)\n",
            "  Downloading msal-1.31.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting msal-extensions>=1.2.0 (from azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-inference)\n",
            "  Downloading msal_extensions-1.2.0-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-azure-inference) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-azure-inference) (2.0.36)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-azure-inference) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-azure-inference) (1.2.15)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-azure-inference) (1.0.8)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-azure-inference) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-azure-inference) (2024.10.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-azure-inference) (0.27.2)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-azure-inference) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-azure-inference) (3.4.2)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-azure-inference) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-azure-inference) (1.26.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-azure-inference) (11.0.0)\n",
            "Requirement already satisfied: pydantic<2.10.0,>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-azure-inference) (2.9.2)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-azure-inference) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-azure-inference) (8.5.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-azure-inference) (0.8.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-azure-inference) (4.66.6)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-azure-inference) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-azure-inference) (1.16.0)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from azure-core>=1.30.0->azure-ai-inference>=1.0.0b5->llama-index-llms-azure-inference) (1.16.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=2.5->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-inference) (1.17.1)\n",
            "Requirement already satisfied: PyJWT<3,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from PyJWT[crypto]<3,>=1.0.0->msal>=1.30.0->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-inference) (2.10.0)\n",
            "Collecting portalocker<3,>=1.4 (from msal-extensions>=1.2.0->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-inference)\n",
            "  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-azure-inference) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-azure-inference) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-azure-inference) (2024.9.11)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2.10.0,>=2.7.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-azure-inference) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<2.10.0,>=2.7.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-azure-inference) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-azure-inference) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-azure-inference) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-azure-inference) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-azure-inference) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-azure-inference) (3.1.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-azure-inference) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-azure-inference) (3.23.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-azure-inference) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-azure-inference) (1.0.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-azure-inference) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-azure-inference) (0.14.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=2.5->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-inference) (2.22)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-azure-inference) (24.2)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-azure-inference) (1.2.2)\n",
            "Downloading llama_index_llms_azure_inference-0.3.0-py3-none-any.whl (6.2 kB)\n",
            "Downloading azure_ai_inference-1.0.0b6-py3-none-any.whl (115 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_identity-1.19.0-py3-none-any.whl (187 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m187.6/187.6 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_core-1.32.0-py3-none-any.whl (198 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.9/198.9 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading isodate-0.7.2-py3-none-any.whl (22 kB)\n",
            "Downloading msal-1.31.1-py3-none-any.whl (113 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.2/113.2 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading msal_extensions-1.2.0-py3-none-any.whl (19 kB)\n",
            "Downloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: portalocker, isodate, azure-core, azure-ai-inference, msal, msal-extensions, azure-identity, llama-index-llms-azure-inference\n",
            "Successfully installed azure-ai-inference-1.0.0b6 azure-core-1.32.0 azure-identity-1.19.0 isodate-0.7.2 llama-index-llms-azure-inference-0.3.0 msal-1.31.1 msal-extensions-1.2.0 portalocker-2.10.1\n",
            "Collecting llama-index-embeddings-azure-inference\n",
            "  Downloading llama_index_embeddings_azure_inference-0.3.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-embeddings-azure-inference) (3.11.2)\n",
            "Requirement already satisfied: azure-ai-inference>=1.0.0b5 in /usr/local/lib/python3.10/dist-packages (from llama-index-embeddings-azure-inference) (1.0.0b6)\n",
            "Requirement already satisfied: azure-identity<2.0.0,>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-embeddings-azure-inference) (1.19.0)\n",
            "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-embeddings-azure-inference) (0.12.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.10.0->llama-index-embeddings-azure-inference) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.10.0->llama-index-embeddings-azure-inference) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.10.0->llama-index-embeddings-azure-inference) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.10.0->llama-index-embeddings-azure-inference) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.10.0->llama-index-embeddings-azure-inference) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.10.0->llama-index-embeddings-azure-inference) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.10.0->llama-index-embeddings-azure-inference) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.10.0->llama-index-embeddings-azure-inference) (4.0.3)\n",
            "Requirement already satisfied: isodate>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from azure-ai-inference>=1.0.0b5->llama-index-embeddings-azure-inference) (0.7.2)\n",
            "Requirement already satisfied: azure-core>=1.30.0 in /usr/local/lib/python3.10/dist-packages (from azure-ai-inference>=1.0.0b5->llama-index-embeddings-azure-inference) (1.32.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from azure-ai-inference>=1.0.0b5->llama-index-embeddings-azure-inference) (4.12.2)\n",
            "Requirement already satisfied: cryptography>=2.5 in /usr/local/lib/python3.10/dist-packages (from azure-identity<2.0.0,>=1.15.0->llama-index-embeddings-azure-inference) (43.0.3)\n",
            "Requirement already satisfied: msal>=1.30.0 in /usr/local/lib/python3.10/dist-packages (from azure-identity<2.0.0,>=1.15.0->llama-index-embeddings-azure-inference) (1.31.1)\n",
            "Requirement already satisfied: msal-extensions>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from azure-identity<2.0.0,>=1.15.0->llama-index-embeddings-azure-inference) (1.2.0)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-inference) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-inference) (2.0.36)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-inference) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-inference) (1.2.15)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-inference) (1.0.8)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-inference) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-inference) (2024.10.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-inference) (0.27.2)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-inference) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-inference) (3.4.2)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-inference) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-inference) (1.26.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-inference) (11.0.0)\n",
            "Requirement already satisfied: pydantic<2.10.0,>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-inference) (2.9.2)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-inference) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-inference) (8.5.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-inference) (0.8.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-inference) (4.66.6)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-inference) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-inference) (1.16.0)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from azure-core>=1.30.0->azure-ai-inference>=1.0.0b5->llama-index-embeddings-azure-inference) (1.16.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=2.5->azure-identity<2.0.0,>=1.15.0->llama-index-embeddings-azure-inference) (1.17.1)\n",
            "Requirement already satisfied: PyJWT<3,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from PyJWT[crypto]<3,>=1.0.0->msal>=1.30.0->azure-identity<2.0.0,>=1.15.0->llama-index-embeddings-azure-inference) (2.10.0)\n",
            "Requirement already satisfied: portalocker<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from msal-extensions>=1.2.0->azure-identity<2.0.0,>=1.15.0->llama-index-embeddings-azure-inference) (2.10.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-inference) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-inference) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-inference) (2024.9.11)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2.10.0,>=2.7.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-inference) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<2.10.0,>=2.7.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-inference) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-inference) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-inference) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-inference) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-inference) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-inference) (3.1.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-inference) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-inference) (3.23.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-inference) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-inference) (1.0.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-inference) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-inference) (0.14.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=2.5->azure-identity<2.0.0,>=1.15.0->llama-index-embeddings-azure-inference) (2.22)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-inference) (24.2)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-inference) (1.2.2)\n",
            "Downloading llama_index_embeddings_azure_inference-0.3.0-py3-none-any.whl (4.0 kB)\n",
            "Installing collected packages: llama-index-embeddings-azure-inference\n",
            "Successfully installed llama-index-embeddings-azure-inference-0.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.azure_inference import AzureAICompletionsModel\n",
        "from llama_index.embeddings.azure_inference import AzureAIEmbeddingsModel"
      ],
      "metadata": {
        "id": "7vjPHsy9ya1H"
      },
      "id": "7vjPHsy9ya1H",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import AzureOpenAI"
      ],
      "metadata": {
        "id": "DLU7kMqyCTMR"
      },
      "id": "DLU7kMqyCTMR",
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = AzureOpenAI(\n",
        "  api_key = \"EcpnZTlxYcDNtgfui7mWYS0JeTWJPlNXJEaILe2fwF6dftfTBqYLJQQJ99ALACYeBjFXJ3w3AAAAACOGXsqG\",\n",
        "  api_version = \"2024-02-01\",\n",
        "  azure_endpoint =\"https://ai-depoc1aihub1643128651037.cognitiveservices.azure.com/\"\n",
        ")\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-35-turbo\", # model = \"deployment_name\".\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"Assistant is a large language model trained by OpenAI.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Who were the founders of Microsoft?\"}\n",
        "    ]\n",
        ")\n",
        "\n",
        "response = client.embeddings.create(\n",
        "    input = \"Your text string goes here\",\n",
        "    model= \"text-embedding-3-large\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "OSJ_36QHCXzB"
      },
      "id": "OSJ_36QHCXzB",
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = AzureAICompletionsModel(\n",
        "    endpoint=\"https://ai-depoc1aihub1643128651037.openai.azure.com/openai/deployments/gpt-4o\",\n",
        "    credential=\"EcpnZTlxYcDNtgfui7mWYS0JeTWJPlNXJEaILe2fwF6dftfTBqYLJQQJ99ALACYeBjFXJ3w3AAAAACOGXsqG\",\n",
        "    api_version=\"2024-08-01-preview\",\n",
        ")"
      ],
      "metadata": {
        "id": "xv3ddXb_ynlf"
      },
      "id": "xv3ddXb_ynlf",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_model = AzureAIEmbeddingsModel(\n",
        "    endpoint=\"https://ai-depoc1aihub1643128651037.openai.azure.com/openai/deployments/text-embedding-3-large\",\n",
        "    credential=\"EcpnZTlxYcDNtgfui7mWYS0JeTWJPlNXJEaILe2fwF6dftfTBqYLJQQJ99ALACYeBjFXJ3w3AAAAACOGXsqG\",\n",
        "    model_name=\"text-embedding-3-large\",\n",
        ")"
      ],
      "metadata": {
        "id": "Ut8t1H4bysOm"
      },
      "id": "Ut8t1H4bysOm",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import Settings"
      ],
      "metadata": {
        "id": "U-JRfQK1zNme"
      },
      "id": "U-JRfQK1zNme",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Settings.llm = llm\n",
        "Settings.embed_model = embed_model"
      ],
      "metadata": {
        "id": "RviQ7kRJzI3c"
      },
      "id": "RviQ7kRJzI3c",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75a05e99-56e2-4db9-baae-f9401100dcc3",
      "metadata": {
        "id": "75a05e99-56e2-4db9-baae-f9401100dcc3"
      },
      "outputs": [],
      "source": [
        "#from llama_index.core import Settings\n",
        "#from llama_index.llms.openai import OpenAI\n",
        "#from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "#\n",
        "#embed_model = OpenAIEmbedding(model=\"text-embedding-3-large\")\n",
        "#llm = OpenAI(model=\"gpt-4o\")\n",
        "#\n",
        "#Settings.embed_model = embed_model\n",
        "#Settings.llm = llm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f16859f-c69e-4edf-acb6-0a5a0784275a",
      "metadata": {
        "id": "2f16859f-c69e-4edf-acb6-0a5a0784275a"
      },
      "source": [
        "#### Parse Documents"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from llama_parse import LlamaParse\n",
        "from getpass import getpass"
      ],
      "metadata": {
        "id": "MqSeOnUm2WfD"
      },
      "id": "MqSeOnUm2WfD",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# API access to llama-cloud\n",
        "os.environ[\"LLAMA_CLOUD_API_KEY\"] = (\n",
        "    os.environ[\"LLAMA_CLOUD_API_KEY\"]\n",
        "    if \"LLAMA_CLOUD_API_KEY\" in os.environ\n",
        "    else getpass(\"LLAMA CLOUD API key: \")\n",
        ")"
      ],
      "metadata": {
        "id": "-FqjMXYZ16tl",
        "outputId": "0a00b446-ae56-4300-eefe-d99c3e93a687",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "-FqjMXYZ16tl",
      "execution_count": 17,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LLAMA CLOUD API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "d6cd2cc9-673f-4f53-81fb-cc990950d345",
      "metadata": {
        "id": "d6cd2cc9-673f-4f53-81fb-cc990950d345"
      },
      "outputs": [],
      "source": [
        "from llama_parse import LlamaParse\n",
        "\n",
        "parser = LlamaParse(result_type=\"markdown\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "papers = [\n",
        "    # \"metagpt.pdf\",\n",
        "    # \"longlora.pdf\",\n",
        "    # \"loftq.pdf\",\n",
        "    \"1.3.2.6.2_Honeywell Long Term Contract_C14.pdf\",\n",
        "    #\"selfrag.pdf\",\n",
        "    # \"zipformer.pdf\",\n",
        "    # \"values.pdf\",\n",
        "    # \"finetune_fair_diffusion.pdf\",\n",
        "    # \"knowledge_card.pdf\",\n",
        "    #\"metra.pdf\",\n",
        "    # \"vr_mcl.pdf\",\n",
        "]"
      ],
      "metadata": {
        "id": "zRm1UDeO2yhF"
      },
      "id": "zRm1UDeO2yhF",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "f9d6f0e8-323e-4786-a4a8-e393441ecd61",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9d6f0e8-323e-4786-a4a8-e393441ecd61",
        "outputId": "3174b441-bc73-496d-ec3d-9c7a7538de8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Started parsing the file under job_id 15d59247-5bad-4979-bbef-7c8f5e08df9d\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "paper_dicts = {}\n",
        "\n",
        "for paper_path in papers:\n",
        "    paper_base = Path(paper_path).stem\n",
        "    full_paper_path = str(Path(data_dir) / paper_path)\n",
        "    md_json_objs = parser.get_json_result(full_paper_path)\n",
        "    json_dicts = md_json_objs[0][\"pages\"]\n",
        "    paper_dicts[paper_path] = {\n",
        "        \"paper_path\": full_paper_path,\n",
        "        \"json_dicts\": json_dicts,\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d52878b-aabf-418e-a4c7-9903a77dd8c8",
      "metadata": {
        "id": "2d52878b-aabf-418e-a4c7-9903a77dd8c8"
      },
      "source": [
        "#### Get Text Nodes\n",
        "\n",
        "Convert the dictionary above into TextNode objects that we can put into a vector store."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "18c24174-05ce-417f-8dd2-79c3f375db03",
      "metadata": {
        "id": "18c24174-05ce-417f-8dd2-79c3f375db03"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.schema import TextNode\n",
        "from typing import Optional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "8e331dfe-a627-4e23-8c57-70ab1d9342e4",
      "metadata": {
        "id": "8e331dfe-a627-4e23-8c57-70ab1d9342e4"
      },
      "outputs": [],
      "source": [
        "# NOTE: these are utility functions to sort the dumped images by the page number\n",
        "# (they are formatted like \"{uuid}-{page_num}.jpg\"\n",
        "import re\n",
        "\n",
        "\n",
        "def get_page_number(file_name):\n",
        "    match = re.search(r\"-page-(\\d+)\\.jpg$\", str(file_name))\n",
        "    if match:\n",
        "        return int(match.group(1))\n",
        "    return 0\n",
        "\n",
        "\n",
        "def _get_sorted_image_files(image_dir):\n",
        "    \"\"\"Get image files sorted by page.\"\"\"\n",
        "    raw_files = [f for f in list(Path(image_dir).iterdir()) if f.is_file()]\n",
        "    sorted_files = sorted(raw_files, key=get_page_number)\n",
        "    return sorted_files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "346fe5ef-171e-4a54-9084-7a7805103a13",
      "metadata": {
        "id": "346fe5ef-171e-4a54-9084-7a7805103a13"
      },
      "outputs": [],
      "source": [
        "from copy import deepcopy\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "# attach image metadata to the text nodes\n",
        "def get_text_nodes(json_dicts, paper_path):\n",
        "    \"\"\"Split docs into nodes, by separator.\"\"\"\n",
        "    nodes = []\n",
        "\n",
        "    md_texts = [d[\"md\"] for d in json_dicts]\n",
        "\n",
        "    for idx, md_text in enumerate(md_texts):\n",
        "        chunk_metadata = {\n",
        "            \"page_num\": idx + 1,\n",
        "            \"paper_path\": paper_path,\n",
        "        }\n",
        "        node = TextNode(\n",
        "            text=md_text,\n",
        "            metadata=chunk_metadata,\n",
        "        )\n",
        "        nodes.append(node)\n",
        "\n",
        "    return nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "f591669c-5a8e-491d-9cef-0b754abbf26f",
      "metadata": {
        "id": "f591669c-5a8e-491d-9cef-0b754abbf26f"
      },
      "outputs": [],
      "source": [
        "# this will combine all nodes from all papers into a single list\n",
        "all_text_nodes = []\n",
        "text_nodes_dict = {}\n",
        "for paper_path, paper_dict in paper_dicts.items():\n",
        "    json_dicts = paper_dict[\"json_dicts\"]\n",
        "    text_nodes = get_text_nodes(json_dicts, paper_dict[\"paper_path\"])\n",
        "    all_text_nodes.extend(text_nodes)\n",
        "    text_nodes_dict[paper_path] = text_nodes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b25f253-3aa0-4689-be6e-d0c722b8b48c",
      "metadata": {
        "id": "3b25f253-3aa0-4689-be6e-d0c722b8b48c"
      },
      "source": [
        "## Add Section Metadata\n",
        "\n",
        "The first step is to extract out a map of all sections from the text of each document. We create a workflow that extracts out if a section heading exists on each page, and merges it together into a combined list. We then run a reflection step to review/correct the extracted sections to make sure everything is correct.\n",
        "\n",
        "Once we have a map of all the sections and the page numbers they start at, we can add the appropriate section ID as metadata to each chunk."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8fdb689-cc94-4da2-ba12-9267e8ee8623",
      "metadata": {
        "id": "d8fdb689-cc94-4da2-ba12-9267e8ee8623"
      },
      "source": [
        "#### Define Section Schema to Extract Into\n",
        "\n",
        "Here we define the output schema which allows us to extract out the section metadata from each section of the document. This will give us a full table of contents of each section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "66358783-1d7f-489d-a85b-35bcb9620912",
      "metadata": {
        "id": "66358783-1d7f-489d-a85b-35bcb9620912"
      },
      "outputs": [],
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from typing import List, Optional\n",
        "\n",
        "\n",
        "class SectionOutput(BaseModel):\n",
        "    \"\"\"The metadata for a given section. Includes the section name, title, page that it starts on, and more.\"\"\"\n",
        "\n",
        "    section_name: str = Field(\n",
        "        ..., description=\"The current section number (e.g. section_name='3.2')\"\n",
        "    )\n",
        "    section_title: str = Field(\n",
        "        ...,\n",
        "        description=\"The current section title associated with the number (e.g. section_title='Experimental Results')\",\n",
        "    )\n",
        "\n",
        "    start_page_number: int = Field(..., description=\"The start page number.\")\n",
        "    is_subsection: bool = Field(\n",
        "        ...,\n",
        "        description=\"True if it's a subsection (e.g. Section 3.2). False if it's not a subsection (e.g. Section 3)\",\n",
        "    )\n",
        "    description: Optional[str] = Field(\n",
        "        None,\n",
        "        description=\"The extracted line from the source text that indicates this is a relevant section.\",\n",
        "    )\n",
        "\n",
        "    def get_section_id(self):\n",
        "        \"\"\"Get section id.\"\"\"\n",
        "        return f\"{self.section_name}: {self.section_title}\"\n",
        "\n",
        "\n",
        "class SectionsOutput(BaseModel):\n",
        "    \"\"\"A list of all sections.\"\"\"\n",
        "\n",
        "    sections: List[SectionOutput]\n",
        "\n",
        "\n",
        "class ValidSections(BaseModel):\n",
        "    \"\"\"A list of indexes, each corresponding to a valid section.\"\"\"\n",
        "\n",
        "    valid_indexes: List[int] = Field(\n",
        "        \"List of valid section indexes. Do NOT include sections to remove.\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bff90c77-f92e-4c5e-a441-70f81adb68fb",
      "metadata": {
        "id": "bff90c77-f92e-4c5e-a441-70f81adb68fb"
      },
      "source": [
        "#### Extract into Section Outputs\n",
        "\n",
        "Use LlamaIndex structured output capabilities to iterate through each page and extract out relevant section metadata. Note: some pages may contain no section metadata (there are no sections that begin on that page)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "dcfcd3a6-4739-4624-a6ed-678e41119575",
      "metadata": {
        "id": "dcfcd3a6-4739-4624-a6ed-678e41119575"
      },
      "outputs": [],
      "source": [
        "#from llama_index.llms.openai import OpenAI\n",
        "from llama_index.core.prompts import ChatPromptTemplate, ChatMessage\n",
        "from llama_index.core.llms import LLM\n",
        "from llama_index.core.async_utils import run_jobs, asyncio_run\n",
        "import json\n",
        "\n",
        "\n",
        "async def aget_sections(\n",
        "    doc_text: str, llm: Optional[LLM] = None\n",
        ") -> List[SectionOutput]:\n",
        "    \"\"\"Get extracted sections from a provided text.\"\"\"\n",
        "\n",
        "    system_prompt = \"\"\"\\\n",
        "    You are an AI document assistant tasked with extracting out section metadata from a document text.\n",
        "\n",
        "- You should ONLY extract out metadata if the document text contains the beginning of a section.\n",
        "- The metadata schema is listed below - you should extract out the section_name, section_title, start page number, description.\n",
        "- A valid section MUST begin with a hashtag (#) and have a number (e.g. \"1 Introduction\" or \"Section 1 Introduction\"). \\\n",
        "Note: Not all hashtag (#) lines are valid sections.\n",
        "\n",
        "- You can extract out multiple section metadata if there are multiple sections on the page.\n",
        "- If there are no sections that begin in this document text, do NOT extract out any sections.\n",
        "- A valid section MUST be clearly delineated in the document text. Do NOT extract out a section if it is mentioned, \\\n",
        "but is not actually the start of a section in the document text.\n",
        "- A Figure or Table does NOT count as a section.\n",
        "\n",
        "    The user will give the document text below.\n",
        "\n",
        "    \"\"\"\n",
        "    llm = llm\n",
        "    #or OpenAI(model=\"gpt-4o\")\n",
        "\n",
        "    chat_template = ChatPromptTemplate(\n",
        "        [\n",
        "            ChatMessage.from_str(system_prompt, \"system\"),\n",
        "            ChatMessage.from_str(\"Document text: {doc_text}\", \"user\"),\n",
        "        ]\n",
        "    )\n",
        "    result = await llm.astructured_predict(\n",
        "        SectionsOutput, chat_template, doc_text=doc_text\n",
        "    )\n",
        "    return result.sections\n",
        "\n",
        "\n",
        "async def arefine_sections(\n",
        "    sections: List[SectionOutput], llm: Optional[LLM] = None\n",
        ") -> List[SectionOutput]:\n",
        "    \"\"\"Refine sections based on extracted text.\"\"\"\n",
        "\n",
        "    system_prompt = \"\"\"\\\n",
        "    You are an AI review assistant tasked with reviewing and correcting another agent's work in extracting sections from a document.\n",
        "\n",
        "    Below is the list of sections with indexes. The sections may be incorrect in the following manner:\n",
        "    - There may be false positive sections - some sections may be wrongly extracted - you can tell by the sequential order of the rest of the sections\n",
        "    - Some sections may be incorrectly marked as subsections and vice-versa\n",
        "    - You can use the description which contains extracted text from the source document to see if it actually qualifies as a section.\n",
        "\n",
        "    Given this, return the list of indexes that are valid. Do NOT include the indexes to be removed.\n",
        "\n",
        "    \"\"\"\n",
        "    llm = llm\n",
        "    #or OpenAI(model=\"gpt-4o\")\n",
        "\n",
        "    chat_template = ChatPromptTemplate(\n",
        "        [\n",
        "            ChatMessage.from_str(system_prompt, \"system\"),\n",
        "            ChatMessage.from_str(\"Sections in text:\\n\\n{sections}\", \"user\"),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    section_texts = \"\\n\".join(\n",
        "        [f\"{idx}: {json.dumps(s.dict())}\" for idx, s in enumerate(sections)]\n",
        "    )\n",
        "\n",
        "    result = await llm.astructured_predict(\n",
        "        ValidSections, chat_template, sections=section_texts\n",
        "    )\n",
        "    valid_indexes = result.valid_indexes\n",
        "\n",
        "    new_sections = [s for idx, s in enumerate(sections) if idx in valid_indexes]\n",
        "    return new_sections\n",
        "\n",
        "\n",
        "async def acreate_sections(text_nodes_dict):\n",
        "    sections_dict = {}\n",
        "    for paper_path, text_nodes in text_nodes_dict.items():\n",
        "        all_sections = []\n",
        "\n",
        "        tasks = [aget_sections(n.get_content(metadata_mode=\"all\")) for n in text_nodes]\n",
        "\n",
        "        async_results = await run_jobs(tasks, workers=8, show_progress=True)\n",
        "        all_sections = [s for r in async_results for s in r]\n",
        "\n",
        "        all_sections = await arefine_sections(all_sections)\n",
        "        sections_dict[paper_path] = all_sections\n",
        "    return sections_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "6e360a5c-29bd-4d86-9a21-f46013bab39a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "id": "6e360a5c-29bd-4d86-9a21-f46013bab39a",
        "outputId": "404444b3-6c05-43ea-bbc9-b18ea1b4cab5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'astructured_predict'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-4073e1fc42d8>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msections_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macreate_sections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_nodes_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/core/async_utils.py\u001b[0m in \u001b[0;36masyncio_run\u001b[0;34m(coro)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# If we're here, there's an existing loop but it's not running\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoro\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36mrun_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     96\u001b[0m                 raise RuntimeError(\n\u001b[1;32m     97\u001b[0m                     'Event loop stopped before Future completed.')\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/asyncio/futures.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__log_traceback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/asyncio/tasks.py\u001b[0m in \u001b[0;36m__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    230\u001b[0m                 \u001b[0;31m# We use the `send` method directly, because coroutines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[0;31m# don't have `__iter__` and `__next__` methods.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-41-2f1a3bfb0603>\u001b[0m in \u001b[0;36macreate_sections\u001b[0;34m(text_nodes_dict)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mtasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0maget_sections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetadata_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"all\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext_nodes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0masync_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mrun_jobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_progress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mall_sections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0masync_results\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\u001b[0m in \u001b[0;36masync_wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    365\u001b[0m             )\n\u001b[1;32m    366\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSpanDropEvent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspan_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mid_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_str\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/core/async_utils.py\u001b[0m in \u001b[0;36mrun_jobs\u001b[0;34m(jobs, show_progress, workers, desc)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masyncio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm_asyncio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mtqdm_asyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpool_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpool_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/asyncio.py\u001b[0m in \u001b[0;36mgather\u001b[0;34m(cls, loop, timeout, total, *fs, **tqdm_kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mifs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mwrap_awaitable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         res = [await f for f in cls.as_completed(ifs, loop=loop, timeout=timeout,\n\u001b[0m\u001b[1;32m     80\u001b[0m                                                  total=total, **tqdm_kwargs)]\n\u001b[1;32m     81\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/asyncio.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mifs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mwrap_awaitable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         res = [await f for f in cls.as_completed(ifs, loop=loop, timeout=timeout,\n\u001b[0m\u001b[1;32m     80\u001b[0m                                                  total=total, **tqdm_kwargs)]\n\u001b[1;32m     81\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/asyncio/tasks.py\u001b[0m in \u001b[0;36m_wait_for_one\u001b[0;34m()\u001b[0m\n\u001b[1;32m    569\u001b[0m             \u001b[0;31m# Dummy value from _on_timeout().\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# May raise f.exception().\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtodo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/asyncio/futures.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__log_traceback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/asyncio/tasks.py\u001b[0m in \u001b[0;36m__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    230\u001b[0m                 \u001b[0;31m# We use the `send` method directly, because coroutines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[0;31m# don't have `__iter__` and `__next__` methods.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/asyncio.py\u001b[0m in \u001b[0;36mwrap_awaitable\u001b[0;34m(i, f)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \"\"\"\n\u001b[1;32m     75\u001b[0m         \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mwrap_awaitable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mifs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mwrap_awaitable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\u001b[0m in \u001b[0;36masync_wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    365\u001b[0m             )\n\u001b[1;32m    366\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSpanDropEvent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspan_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mid_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_str\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/core/async_utils.py\u001b[0m in \u001b[0;36mworker\u001b[0;34m(job)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mworker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCoroutine\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32masync\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0msemaphore\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mjob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0mpool_jobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mworker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mjob\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjobs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-41-2f1a3bfb0603>\u001b[0m in \u001b[0;36maget_sections\u001b[0;34m(doc_text, llm)\u001b[0m\n\u001b[1;32m     37\u001b[0m         ]\n\u001b[1;32m     38\u001b[0m     )\n\u001b[0;32m---> 39\u001b[0;31m     result = await llm.astructured_predict(\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mSectionsOutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchat_template\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdoc_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     )\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'astructured_predict'"
          ]
        }
      ],
      "source": [
        "sections_dict = asyncio_run(acreate_sections(text_nodes_dict))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d930f0e5-5295-46b0-b54b-e2da4fb25fe5",
      "metadata": {
        "id": "d930f0e5-5295-46b0-b54b-e2da4fb25fe5",
        "outputId": "037304a5-dd3c-417a-e197-8b5170e41d0e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[SectionOutput(section_name='1', section_title='INTRODUCTION', start_page_number=1, is_subsection=False, description='# 1 INTRODUCTION'),\n",
              " SectionOutput(section_name='2', section_title='BENCHMARK CONSTRUCTION', start_page_number=2, is_subsection=False, description='# BENCHMARK CONSTRUCTION'),\n",
              " SectionOutput(section_name='2.2', section_title='TASK FORMULATION', start_page_number=3, is_subsection=True, description='# 2.2 TASK FORMULATION'),\n",
              " SectionOutput(section_name='2.3', section_title='FEATURES OF SWE-BENCH', start_page_number=3, is_subsection=True, description='# 2.3 FEATURES OF SWE-BENCH'),\n",
              " SectionOutput(section_name='3', section_title='SWE-LLAMA: FINE-TUNING CODELLAMA FOR SWE-BENCH', start_page_number=3, is_subsection=False, description='# 3 SWE-LLAMA: FINE-TUNING CODELLAMA FOR SWE-BENCH'),\n",
              " SectionOutput(section_name='4', section_title='EXPERIMENTAL SETUP', start_page_number=4, is_subsection=False, description='# 4 EXPERIMENTAL SETUP'),\n",
              " SectionOutput(section_name='4.1', section_title='RETRIEVAL-BASED APPROACH', start_page_number=4, is_subsection=True, description='# 4.1 RETRIEVAL-BASED APPROACH'),\n",
              " SectionOutput(section_name='4.2', section_title='INPUT FORMAT', start_page_number=5, is_subsection=True, description='# 4.2 INPUT FORMAT'),\n",
              " SectionOutput(section_name='4.3', section_title='MODELS', start_page_number=5, is_subsection=True, description='# 4.3 MODELS'),\n",
              " SectionOutput(section_name='5', section_title='RESULTS', start_page_number=5, is_subsection=False, description='# 5 RESULTS'),\n",
              " SectionOutput(section_name='5.1', section_title='A QUALITATIVE ANALYSIS OF SWE-LLAMA GENERATIONS', start_page_number=8, is_subsection=True, description='# 5.1 A QUALITATIVE ANALYSIS OF SWE-LLAMA GENERATIONS'),\n",
              " SectionOutput(section_name='6', section_title='RELATED WORK', start_page_number=8, is_subsection=False, description='# 6 RELATED WORK'),\n",
              " SectionOutput(section_name='7', section_title='DISCUSSION', start_page_number=9, is_subsection=False, description='# 7 DISCUSSION'),\n",
              " SectionOutput(section_name='8', section_title='ETHICS STATEMENT', start_page_number=10, is_subsection=False, description='# 8 ETHICS STATEMENT'),\n",
              " SectionOutput(section_name='9', section_title='REPRODUCIBILITY STATEMENT', start_page_number=10, is_subsection=False, description='# 9 REPRODUCIBILITY STATEMENT'),\n",
              " SectionOutput(section_name='10', section_title='ACKNOWLEDGEMENTS', start_page_number=10, is_subsection=False, description='# 10 ACKNOWLEDGEMENTS'),\n",
              " SectionOutput(section_name='A', section_title='BENCHMARK DETAILS', start_page_number=15, is_subsection=False, description='# A BENCHMARK DETAILS'),\n",
              " SectionOutput(section_name='A.1', section_title='HIGH LEVEL OVERVIEW', start_page_number=15, is_subsection=True, description='# A.1 HIGH LEVEL OVERVIEW'),\n",
              " SectionOutput(section_name='A.2', section_title='CONSTRUCTION PROCESS', start_page_number=16, is_subsection=True, description='# A.2 CONSTRUCTION PROCESS'),\n",
              " SectionOutput(section_name='A.3', section_title='Execution-Based Validation', start_page_number=18, is_subsection=True, description='# A.3 EXECUTION-BASED VALIDATION'),\n",
              " SectionOutput(section_name='A.5', section_title='Evaluation Test Set Characterization', start_page_number=20, is_subsection=True, description='# A.5 EVALUATION TEST SET CHARACTERIZATION'),\n",
              " SectionOutput(section_name='A.6', section_title='DEVELOPMENT SET CHARACTERIZATION', start_page_number=23, is_subsection=True, description='# A.6 DEVELOPMENT SET CHARACTERIZATION'),\n",
              " SectionOutput(section_name='B', section_title='ADDITIONAL DETAILS ON TRAINING SWE-LLAMA', start_page_number=24, is_subsection=False, description='# B ADDITIONAL DETAILS ON TRAINING SWE-LLAMA'),\n",
              " SectionOutput(section_name='B.1', section_title='TRAINING DETAILS', start_page_number=24, is_subsection=True, description='# B.1 TRAINING DETAILS'),\n",
              " SectionOutput(section_name='D', section_title='ADDITIONAL EXPERIMENTAL DETAILS', start_page_number=28, is_subsection=False, description='# D ADDITIONAL EXPERIMENTAL DETAILS'),\n",
              " SectionOutput(section_name='D.1', section_title='RETRIEVAL DETAILS', start_page_number=28, is_subsection=True, description='# D.1 RETRIEVAL DETAILS'),\n",
              " SectionOutput(section_name='D.2', section_title='INFERENCE SETTINGS', start_page_number=29, is_subsection=True, description='# D.2 INFERENCE SETTINGS'),\n",
              " SectionOutput(section_name='D.3', section_title='PROMPT TEMPLATE EXAMPLE', start_page_number=29, is_subsection=True, description='# D.3 PROMPT TEMPLATE EXAMPLE'),\n",
              " SectionOutput(section_name='E', section_title='Societal Impact', start_page_number=31, is_subsection=False, description='# E SOCIETAL IMPACT'),\n",
              " SectionOutput(section_name='F', section_title='In-Depth Analysis of SWE-Llama Generations', start_page_number=31, is_subsection=False, description='# F IN-DEPTH ANALYSIS OF SWE-LLAMA GENERATIONS')]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sections_dict[\"swebench.pdf\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c6f237b-df2a-4d9e-91bf-b0bbb88ef183",
      "metadata": {
        "id": "8c6f237b-df2a-4d9e-91bf-b0bbb88ef183"
      },
      "outputs": [],
      "source": [
        "# [Optional] SAVE\n",
        "import pickle\n",
        "\n",
        "pickle.dump(sections_dict, open(\"sections_dict.pkl\", \"wb\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7497b614-250e-4f3e-8940-b361996a00b6",
      "metadata": {
        "id": "7497b614-250e-4f3e-8940-b361996a00b6"
      },
      "outputs": [],
      "source": [
        "# [Optional] LOAD\n",
        "sections_dict = pickle.load(open(\"sections_dict.pkl\", \"rb\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28b01141-d9c1-424c-937a-8707867180b1",
      "metadata": {
        "id": "28b01141-d9c1-424c-937a-8707867180b1"
      },
      "source": [
        "#### Annotate each chunk with the section metadata\n",
        "\n",
        "In the section above we've extracted out a TOC of all sections/subsections and their page numbers. Given this we can just do one forward pass through all the chunks, and annotate them with the section they correspond to (e.g. the section/subsection with the highest page number less than the page number of the chunk)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38133abe-7800-424a-9259-71df6d154d31",
      "metadata": {
        "id": "38133abe-7800-424a-9259-71df6d154d31"
      },
      "outputs": [],
      "source": [
        "def annotate_chunks_with_sections(chunks, sections):\n",
        "    main_sections = [s for s in sections if not s.is_subsection]\n",
        "    # subsections include the main sections too (some sections have no subsections etc.)\n",
        "    sub_sections = sections\n",
        "\n",
        "    main_section_idx, sub_section_idx = 0, 0\n",
        "    for idx, c in enumerate(chunks):\n",
        "        cur_page = c.metadata[\"page_num\"]\n",
        "        while (\n",
        "            main_section_idx + 1 < len(main_sections)\n",
        "            and main_sections[main_section_idx + 1].start_page_number <= cur_page\n",
        "        ):\n",
        "            main_section_idx += 1\n",
        "        while (\n",
        "            sub_section_idx + 1 < len(sub_sections)\n",
        "            and sub_sections[sub_section_idx + 1].start_page_number <= cur_page\n",
        "        ):\n",
        "            sub_section_idx += 1\n",
        "\n",
        "        cur_main_section = main_sections[main_section_idx]\n",
        "        cur_sub_section = sub_sections[sub_section_idx]\n",
        "\n",
        "        c.metadata[\"section_id\"] = cur_main_section.get_section_id()\n",
        "        c.metadata[\"sub_section_id\"] = cur_sub_section.get_section_id()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d125b0c0-acb0-4f56-9ef3-f06d452ae3cd",
      "metadata": {
        "id": "d125b0c0-acb0-4f56-9ef3-f06d452ae3cd"
      },
      "outputs": [],
      "source": [
        "for paper_path, text_nodes in text_nodes_dict.items():\n",
        "    sections = sections_dict[paper_path]\n",
        "    annotate_chunks_with_sections(text_nodes, sections)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1ab80d2-cac4-417d-aaac-7ea9dfed49f7",
      "metadata": {
        "id": "b1ab80d2-cac4-417d-aaac-7ea9dfed49f7"
      },
      "source": [
        "You can choose to save these nodes if you'd like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2272ae05-89f6-46a9-9b9f-915e15908128",
      "metadata": {
        "id": "2272ae05-89f6-46a9-9b9f-915e15908128"
      },
      "outputs": [],
      "source": [
        "# SAVE\n",
        "import pickle\n",
        "\n",
        "pickle.dump(text_nodes_dict, open(\"iclr_text_nodes.pkl\", \"wb\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ebf0173-af45-4fae-aca4-2ceb266f8357",
      "metadata": {
        "id": "8ebf0173-af45-4fae-aca4-2ceb266f8357"
      },
      "source": [
        "**LOAD**: If you've already saved nodes, run the below cell to load from an existing file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1e5425b-4872-47b3-86f5-f6a068788a2b",
      "metadata": {
        "id": "c1e5425b-4872-47b3-86f5-f6a068788a2b"
      },
      "outputs": [],
      "source": [
        "# LOAD\n",
        "import pickle\n",
        "\n",
        "text_nodes_dict = pickle.load(open(\"iclr_text_nodes.pkl\", \"rb\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "642e90f1-1d32-4925-b37d-2af8a0ca9712",
      "metadata": {
        "id": "642e90f1-1d32-4925-b37d-2af8a0ca9712"
      },
      "outputs": [],
      "source": [
        "all_text_nodes = []\n",
        "for paper_path, text_nodes in text_nodes_dict.items():\n",
        "    all_text_nodes.extend(text_nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d7b566b-5ec1-4e49-b4d4-e863af2aabc6",
      "metadata": {
        "id": "8d7b566b-5ec1-4e49-b4d4-e863af2aabc6",
        "outputId": "969ea6b0-c75d-4930-a01c-db78e5ff87b7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "106"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(all_text_nodes)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d03a4de3-39ce-40c3-b37b-b6bbc597ddb1",
      "metadata": {
        "id": "d03a4de3-39ce-40c3-b37b-b6bbc597ddb1"
      },
      "source": [
        "### Build Indexes\n",
        "\n",
        "Once the text nodes are ready, we feed into our vector store index abstraction, which will index these nodes into a simple in-memory vector store (of course, you should definitely check out our 40+ vector store integrations!)\n",
        "\n",
        "Besides vector indexing, we **also** store a mapping of paper path to the summary index. This allows us to perform document-level retrieval - retrieve all chunks relevant to a given document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "add64e3e-12df-4d5a-beba-b3018325e15b",
      "metadata": {
        "id": "add64e3e-12df-4d5a-beba-b3018325e15b"
      },
      "outputs": [],
      "source": [
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "from llama_index.core import VectorStoreIndex\n",
        "\n",
        "persist_dir = \"storage_chroma\"\n",
        "\n",
        "vector_store = ChromaVectorStore.from_params(\n",
        "    collection_name=\"text_nodes\", persist_dir=persist_dir\n",
        ")\n",
        "index = VectorStoreIndex.from_vector_store(vector_store)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e46583a-6c6b-4a5e-b78a-d06721ae7d1c",
      "metadata": {
        "id": "1e46583a-6c6b-4a5e-b78a-d06721ae7d1c"
      },
      "source": [
        "**NOTE**: Don't run the block below if you've already inserted the nodes. Only run if it's your first time!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9777f302-699a-4417-99b8-2be4e7cd60f5",
      "metadata": {
        "id": "9777f302-699a-4417-99b8-2be4e7cd60f5"
      },
      "outputs": [],
      "source": [
        "index.insert_nodes(all_text_nodes)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d46f14ff-45b1-41f4-84e2-a6e5d6637809",
      "metadata": {
        "id": "d46f14ff-45b1-41f4-84e2-a6e5d6637809"
      },
      "source": [
        "## Setup Dynamic, Section-Level Retrieval\n",
        "\n",
        "We now setup a retriever that will allow us to retrieve an entire contiguous section in a document, instead of a chunk of it. This is useful for preserving the entire context within a doc.\n",
        "\n",
        "- Step 1: Do chunk-level retrieval to find the relevant chunks.\n",
        "- Step 2: For each chunk, identify the section that it corresponds to.\n",
        "- Step 3: Do a second retrieval pass using metadata filters to find the entire contiguous section that matches the chunk, and return that as a continguous node.\n",
        "- Step 4: Feed the contiguous sections into the LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "652cb067-da39-42cb-a303-faa346f72e13",
      "metadata": {
        "id": "652cb067-da39-42cb-a303-faa346f72e13"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "llm = OpenAI(model=\"gpt-4o\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "253f0c57-f5b4-4dbd-a0a0-62a42bd5bbdc",
      "metadata": {
        "id": "253f0c57-f5b4-4dbd-a0a0-62a42bd5bbdc"
      },
      "outputs": [],
      "source": [
        "chunk_retriever = index.as_retriever(similarity_top_k=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0a564cb-bfdb-48a5-9d67-10390c3a6c28",
      "metadata": {
        "id": "b0a564cb-bfdb-48a5-9d67-10390c3a6c28"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.vector_stores.types import (\n",
        "    VectorStoreInfo,\n",
        "    VectorStoreQuerySpec,\n",
        "    MetadataInfo,\n",
        "    MetadataFilters,\n",
        "    FilterCondition,\n",
        ")\n",
        "from llama_index.core.schema import NodeWithScore\n",
        "\n",
        "\n",
        "def section_retrieve(query: str, verbose: bool = False) -> List[NodeWithScore]:\n",
        "    \"\"\"Retrieve sections.\"\"\"\n",
        "    if verbose:\n",
        "        print(f\">> Identifying the right sections to retrieve\")\n",
        "    chunk_nodes = chunk_retriever.retrieve(query)\n",
        "\n",
        "    all_section_nodes = {}\n",
        "    for node in chunk_nodes:\n",
        "        section_id = node.node.metadata[\"section_id\"]\n",
        "        if verbose:\n",
        "            print(f\">> Retrieving section: {section_id}\")\n",
        "        filters = MetadataFilters.from_dicts(\n",
        "            [\n",
        "                {\"key\": \"section_id\", \"value\": section_id, \"operator\": \"==\"},\n",
        "                {\n",
        "                    \"key\": \"paper_path\",\n",
        "                    \"value\": node.node.metadata[\"paper_path\"],\n",
        "                    \"operator\": \"==\",\n",
        "                },\n",
        "            ],\n",
        "            condition=FilterCondition.AND,\n",
        "        )\n",
        "\n",
        "        # TODO: make node_ids not positional\n",
        "        section_nodes_raw = index.vector_store.get_nodes(node_ids=None, filters=filters)\n",
        "        section_nodes = [NodeWithScore(node=n) for n in section_nodes_raw]\n",
        "        # order and consolidate nodes\n",
        "        section_nodes_sorted = sorted(\n",
        "            section_nodes, key=lambda x: x.metadata[\"page_num\"]\n",
        "        )\n",
        "\n",
        "        all_section_nodes.update({n.id_: n for n in section_nodes_sorted})\n",
        "    return all_section_nodes.values()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f721e770-ce4c-4511-96d5-8a89d16c7281",
      "metadata": {
        "id": "f721e770-ce4c-4511-96d5-8a89d16c7281",
        "outputId": "f62d2252-89ae-42c1-8c41-315e104ebbcb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> Identifying the right sections to retrieve\n",
            ">> Retrieving section: A: BENCHMARK DETAILS\n",
            ">> Retrieving section: 2: BENCHMARK CONSTRUCTION\n",
            ">> Retrieving section: A: BENCHMARK DETAILS\n"
          ]
        }
      ],
      "source": [
        "nodes = section_retrieve(\n",
        "    \"Give me a full overview of the benchmark details in SWE Bench\", verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e99eaa71-7d93-40c0-bba0-a9c983a6cbd3",
      "metadata": {
        "id": "e99eaa71-7d93-40c0-bba0-a9c983a6cbd3",
        "outputId": "f3601451-79e6-4d47-d8b7-2e77bf506e0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'page_num': 15, 'paper_path': 'iclr_docs/swebench.pdf', 'section_id': 'A: BENCHMARK DETAILS', 'sub_section_id': 'A.1: HIGH LEVEL OVERVIEW'}\n",
            "{'page_num': 16, 'paper_path': 'iclr_docs/swebench.pdf', 'section_id': 'A: BENCHMARK DETAILS', 'sub_section_id': 'A.2: CONSTRUCTION PROCESS'}\n",
            "{'page_num': 17, 'paper_path': 'iclr_docs/swebench.pdf', 'section_id': 'A: BENCHMARK DETAILS', 'sub_section_id': 'A.2: CONSTRUCTION PROCESS'}\n",
            "{'page_num': 18, 'paper_path': 'iclr_docs/swebench.pdf', 'section_id': 'A: BENCHMARK DETAILS', 'sub_section_id': 'A.3: Execution-Based Validation'}\n",
            "{'page_num': 19, 'paper_path': 'iclr_docs/swebench.pdf', 'section_id': 'A: BENCHMARK DETAILS', 'sub_section_id': 'A.3: Execution-Based Validation'}\n",
            "{'page_num': 20, 'paper_path': 'iclr_docs/swebench.pdf', 'section_id': 'A: BENCHMARK DETAILS', 'sub_section_id': 'A.5: Evaluation Test Set Characterization'}\n",
            "{'page_num': 21, 'paper_path': 'iclr_docs/swebench.pdf', 'section_id': 'A: BENCHMARK DETAILS', 'sub_section_id': 'A.5: Evaluation Test Set Characterization'}\n",
            "{'page_num': 22, 'paper_path': 'iclr_docs/swebench.pdf', 'section_id': 'A: BENCHMARK DETAILS', 'sub_section_id': 'A.5: Evaluation Test Set Characterization'}\n",
            "{'page_num': 23, 'paper_path': 'iclr_docs/swebench.pdf', 'section_id': 'A: BENCHMARK DETAILS', 'sub_section_id': 'A.6: DEVELOPMENT SET CHARACTERIZATION'}\n",
            "{'page_num': 2, 'paper_path': 'iclr_docs/swebench.pdf', 'section_id': '2: BENCHMARK CONSTRUCTION', 'sub_section_id': '2: BENCHMARK CONSTRUCTION'}\n"
          ]
        }
      ],
      "source": [
        "for n in nodes:\n",
        "    print(n.node.metadata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "509de5ae-4d51-4b39-b67e-698cb84acd73",
      "metadata": {
        "id": "509de5ae-4d51-4b39-b67e-698cb84acd73",
        "outputId": "74e990f3-e6b2-4929-f9d9-93d9bc00b6ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> Identifying the right sections to retrieve\n",
            ">> Retrieving section: F: ADDITIONAL RESULTS\n",
            ">> Retrieving section: 5: EXPERIMENTS\n",
            ">> Retrieving section: F: ADDITIONAL RESULTS\n"
          ]
        }
      ],
      "source": [
        "nodes = section_retrieve(\n",
        "    \"Give me details of all additional experimental results in the Metra paper\",\n",
        "    verbose=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db64a838-5f19-46e0-b874-859a125f8dcd",
      "metadata": {
        "id": "db64a838-5f19-46e0-b874-859a125f8dcd",
        "outputId": "5edd609f-6acf-4fa9-e807-aa0619b1ea79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'page_num': 21, 'paper_path': 'iclr_docs/metra.pdf', 'section_id': 'F: ADDITIONAL RESULTS', 'sub_section_id': 'F.1: FULL QUALITATIVE RESULTS'}\n",
            "{'page_num': 22, 'paper_path': 'iclr_docs/metra.pdf', 'section_id': 'F: ADDITIONAL RESULTS', 'sub_section_id': 'F.4: Additional Baselines'}\n",
            "{'page_num': 6, 'paper_path': 'iclr_docs/metra.pdf', 'section_id': '5: EXPERIMENTS', 'sub_section_id': '5: EXPERIMENTS'}\n",
            "{'page_num': 7, 'paper_path': 'iclr_docs/metra.pdf', 'section_id': '5: EXPERIMENTS', 'sub_section_id': '5.2: QUALITATIVE COMPARISON'}\n",
            "{'page_num': 8, 'paper_path': 'iclr_docs/metra.pdf', 'section_id': '5: EXPERIMENTS', 'sub_section_id': '5.3: QUANTITATIVE COMPARISON'}\n"
          ]
        }
      ],
      "source": [
        "for n in nodes:\n",
        "    print(n.node.metadata)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d67303e6-ec65-499b-85bb-8189d220b466",
      "metadata": {
        "id": "d67303e6-ec65-499b-85bb-8189d220b466"
      },
      "source": [
        "### Try out Section-Level Retrieval as a Full RAG Pipeline\n",
        "\n",
        "Now that we've defined the retriever, we can plug the retrieved results into an LLM to create a full RAG pipeline!\n",
        "\n",
        "Our response synthesizers help handle dumping context into the LLM prompt window while accounting for context window limitations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb382809-d38e-4f03-bf26-6e1bf0d98df6",
      "metadata": {
        "id": "bb382809-d38e-4f03-bf26-6e1bf0d98df6"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.query_engine import CustomQueryEngine\n",
        "from llama_index.core.response_synthesizers import TreeSummarize, BaseSynthesizer\n",
        "\n",
        "\n",
        "class SectionRetrieverRAGEngine(CustomQueryEngine):\n",
        "    \"\"\"RAG Query Engine.\"\"\"\n",
        "\n",
        "    synthesizer: BaseSynthesizer\n",
        "    verbose: bool = True\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(synthesizer=TreeSummarize(llm=llm))\n",
        "\n",
        "    def custom_query(self, query_str: str):\n",
        "        nodes = section_retrieve(query_str, verbose=self.verbose)\n",
        "        response_obj = self.synthesizer.synthesize(query_str, nodes)\n",
        "        return response_obj"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "426d9426-a145-4f50-ad37-4dd82b5c7ae8",
      "metadata": {
        "id": "426d9426-a145-4f50-ad37-4dd82b5c7ae8"
      },
      "outputs": [],
      "source": [
        "query_engine = SectionRetrieverRAGEngine()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1ec3f98-7181-4850-8b37-1e0aa751bf54",
      "metadata": {
        "id": "d1ec3f98-7181-4850-8b37-1e0aa751bf54",
        "outputId": "984edcff-5d0e-4a5a-dc35-2b7bfe1142a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> Identifying the right sections to retrieve\n",
            ">> Retrieving section: A: BENCHMARK DETAILS\n",
            ">> Retrieving section: 5: RESULTS\n",
            ">> Retrieving section: A: BENCHMARK DETAILS\n",
            "In SWEBench, difficulty correlates with context length in a way that as the total context length increases, model performance tends to drop. This is observed across various models, including Claude 2, which shows a significant decrease in performance with longer context lengths. The models often struggle to localize the problematic code that needs updating when presented with a lot of code that may not be directly related to the issue at hand. This suggests that models can become distracted by additional context, which aligns with findings from other studies indicating that models may be sensitive to the relative location of target sequences. Even when increasing the maximum context size improves recall with respect to the oracle files, performance still drops, indicating that models are ineffective at localizing the necessary code changes.\n"
          ]
        }
      ],
      "source": [
        "response = query_engine.query(\n",
        "    \"Tell me more about how difficulty correlates with context length in SWEBench\"\n",
        ")\n",
        "print(str(response))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "483f5615-ab58-4bc7-968b-7a9e116756e1",
      "metadata": {
        "id": "483f5615-ab58-4bc7-968b-7a9e116756e1",
        "outputId": "bc353fcf-1152-4e83-b87f-c6f8d8886a88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> Identifying the right sections to retrieve\n",
            ">> Retrieving section: A: BENCHMARK DETAILS\n",
            ">> Retrieving section: 2: BENCHMARK CONSTRUCTION\n",
            ">> Retrieving section: A: BENCHMARK DETAILS\n",
            "SWE-bench is a benchmark designed to evaluate language models in a realistic software engineering setting by using GitHub issues and pull requests from popular repositories. The benchmark involves generating a pull request that addresses a given issue and passes related tests. The construction of SWE-bench involves a three-stage pipeline:\n",
            "\n",
            "1. **Repo Selection and Data Scraping**: Pull requests are collected from 12 popular open-source Python repositories on GitHub, resulting in approximately 90,000 PRs. These repositories are chosen for their better maintenance, clear contributor guidelines, and comprehensive test coverage.\n",
            "\n",
            "2. **Attribute-Based Filtering**: Candidate tasks are created by selecting merged PRs that resolve a GitHub issue and contribute tests. This indicates that the user likely added tests to verify the resolution of the issue.\n",
            "\n",
            "3. **Execution-Based Filtering**: For each candidate task, the PR's test content is applied, and test results are logged before and after applying the PR's other content. Tasks are filtered out if they do not have at least one test that changes from fail to pass or if they result in installation or runtime errors.\n",
            "\n",
            "The benchmark is designed to be extensible, allowing for updates with new task instances as new language models are released. It includes a robust framework for execution-based evaluation, ensuring that generated solutions can be verified by running unit tests. SWE-bench also provides a training dataset, SWE-bench-train, and fine-tuned models like SWE-Llama 7b and 13b, which are based on the CodeLlama model. These models are evaluated on their ability to resolve issues, with SWE-Llama 13b showing competitive performance in some settings.\n"
          ]
        }
      ],
      "source": [
        "response = query_engine.query(\n",
        "    \"Give me a full overview of the benchmark details in SWE Bench\"\n",
        ")\n",
        "print(str(response))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d747bf8-0ed2-4c10-8108-9d0e8d53a4fb",
      "metadata": {
        "id": "6d747bf8-0ed2-4c10-8108-9d0e8d53a4fb",
        "outputId": "c38655b9-dd1b-49d9-cf8d-bb1b2e3f46e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'page_num': 15, 'paper_path': 'iclr_docs/swebench.pdf', 'section_id': 'A: BENCHMARK DETAILS', 'sub_section_id': 'A.1: HIGH LEVEL OVERVIEW'}\n",
            "{'page_num': 16, 'paper_path': 'iclr_docs/swebench.pdf', 'section_id': 'A: BENCHMARK DETAILS', 'sub_section_id': 'A.2: CONSTRUCTION PROCESS'}\n",
            "{'page_num': 17, 'paper_path': 'iclr_docs/swebench.pdf', 'section_id': 'A: BENCHMARK DETAILS', 'sub_section_id': 'A.2: CONSTRUCTION PROCESS'}\n",
            "{'page_num': 18, 'paper_path': 'iclr_docs/swebench.pdf', 'section_id': 'A: BENCHMARK DETAILS', 'sub_section_id': 'A.3: Execution-Based Validation'}\n",
            "{'page_num': 19, 'paper_path': 'iclr_docs/swebench.pdf', 'section_id': 'A: BENCHMARK DETAILS', 'sub_section_id': 'A.3: Execution-Based Validation'}\n",
            "{'page_num': 20, 'paper_path': 'iclr_docs/swebench.pdf', 'section_id': 'A: BENCHMARK DETAILS', 'sub_section_id': 'A.5: Evaluation Test Set Characterization'}\n",
            "{'page_num': 21, 'paper_path': 'iclr_docs/swebench.pdf', 'section_id': 'A: BENCHMARK DETAILS', 'sub_section_id': 'A.5: Evaluation Test Set Characterization'}\n",
            "{'page_num': 22, 'paper_path': 'iclr_docs/swebench.pdf', 'section_id': 'A: BENCHMARK DETAILS', 'sub_section_id': 'A.5: Evaluation Test Set Characterization'}\n",
            "{'page_num': 23, 'paper_path': 'iclr_docs/swebench.pdf', 'section_id': 'A: BENCHMARK DETAILS', 'sub_section_id': 'A.6: DEVELOPMENT SET CHARACTERIZATION'}\n",
            "{'page_num': 2, 'paper_path': 'iclr_docs/swebench.pdf', 'section_id': '2: BENCHMARK CONSTRUCTION', 'sub_section_id': '2: BENCHMARK CONSTRUCTION'}\n"
          ]
        }
      ],
      "source": [
        "for n in response.source_nodes:\n",
        "    print(n.metadata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62b11a23-df6a-4d83-b35c-691bb4d125c0",
      "metadata": {
        "id": "62b11a23-df6a-4d83-b35c-691bb4d125c0",
        "outputId": "0118b99b-c2a2-4cfa-b798-62f213530eed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> Identifying the right sections to retrieve\n",
            ">> Retrieving section: F: ADDITIONAL RESULTS\n",
            ">> Retrieving section: 5: EXPERIMENTS\n",
            ">> Retrieving section: F: ADDITIONAL RESULTS\n",
            "The additional experimental results in the METRA paper include several key findings:\n",
            "\n",
            "1. **Full Qualitative Results**: METRA discovers diverse locomotion behaviors across different environments, including state-based Ant and HalfCheetah, and pixel-based Quadruped and Humanoid. The results are consistent across multiple random seeds, indicating robustness in behavior discovery.\n",
            "\n",
            "2. **Latent Space Visualization**: METRA effectively captures the most temporally spread-out dimensions in the state space, such as x-y coordinates, in its latent space. This is demonstrated in both state-based and pixel-based environments, with higher-dimensional latent spaces capturing more diverse behaviors.\n",
            "\n",
            "3. **Ablation Study of Latent Space Sizes**: The study shows that increasing the size of the latent space generally enhances the diversity of skills learned by METRA. Different dimensions of continuous and discrete skills were tested on Ant and HalfCheetah.\n",
            "\n",
            "4. **Comparison with Additional Baselines**: METRA was compared with DGPO, a method focused on finding diverse behaviors that maximize task rewards. The comparison was conducted in a controlled Markov process setting without external rewards, using only intrinsic rewards.\n",
            "\n",
            "These results highlight METRA's ability to discover diverse and meaningful behaviors in various environments, its effective use of latent spaces, and its performance relative to other methods.\n"
          ]
        }
      ],
      "source": [
        "response = query_engine.query(\n",
        "    \"Give me details of all additional experimental results in the Metra paper\"\n",
        ")\n",
        "print(str(response))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llama_index_v3",
      "language": "python",
      "name": "llama_index_v3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}